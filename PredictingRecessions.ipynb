{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m68nISvE03b5",
        "outputId": "434888aa-e2b5-4289-bdf8-f905bd670908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's Prediction vs Actual Recessions:\n",
            "Accuracy: 98.71%\n",
            "Precision: 98.71%\n",
            "Recall: 98.71%\n",
            "Confusion Matrix:\n",
            "[[427   3]\n",
            " [  3  33]]\n",
            "F1 Score: 98.71%\n",
            "ROC AUC Score: 95.48%\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "KAVE-AI\n",
        "Predicting US Economic Recessions\n",
        "CS 499 - Capstone Project\n",
        "\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score, average_precision_score, make_scorer, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "# Unemployment Rate (01/1985 - 10/2023) 'https://fred.stlouisfed.org/series/UNRATE'\n",
        "unemp_dataset = pd.read_csv('UNRATE.csv')\n",
        "\n",
        "# Consumer Price Index for All Urban Consumers: All Items (01/1985 - 10/2023) 'https://fred.stlouisfed.org/series/CPIAUCSL'\n",
        "cpi_dataset = pd.read_csv('CPIAUCSL.csv')\n",
        "\n",
        "# 10-Year Treasury Constant Maturity Minus 3-Month Treasury Constant Maturity Monthly Avg (01/1985 - 10/2023) https://fred.stlouisfed.org/series/T10Y3M#0\n",
        "t_10y_3m_dataset = pd.read_csv('T10Y3M.csv')\n",
        "\n",
        "# S&P 500 (01/1985 - 10/2023) https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC\n",
        "sp_500_daily_dataset = pd.read_csv('^GSPC.csv')\n",
        "sp_500_daily_dataset.rename(columns={'Date': 'DATE'}, inplace=True)\n",
        "sp_500_daily_dataset['DATE'] = pd.to_datetime(sp_500_daily_dataset['DATE'])\n",
        "sp_500_daily_dataset.set_index('DATE', inplace=True)\n",
        "\n",
        "# we need the mean adj close for each month\n",
        "sp_500_monthly_dataset = sp_500_daily_dataset['Adj Close'].resample('M').mean()\n",
        "\n",
        "start_date = '1985-01-02'\n",
        "end_date = '2023-10-31'\n",
        "\n",
        "sp_500_monthly_dataset = sp_500_monthly_dataset[start_date:end_date]\n",
        "sp_500_monthly_dataset = sp_500_monthly_dataset.reset_index()\n",
        "sp_500_monthly_dataset['DATE'] = sp_500_monthly_dataset['DATE'].dt.to_period(\"M\").dt.strftime(\"%m/%Y\")\n",
        "\n",
        "start_date = '01/1985'\n",
        "\n",
        "sp_500_monthly_dataset = sp_500_monthly_dataset[sp_500_monthly_dataset['DATE'] >= start_date]\n",
        "\n",
        "\n",
        "# save dataframe to a new CSV file, used for test/training\n",
        "sp_500_monthly_dataset.to_csv('SP500.csv', index=False)\n",
        "\n",
        "\n",
        "# Historical dataset (1985 - 2023) 'https://fred.stlouisfed.org/series/USREC#0'\n",
        "historical_recession_dataset = pd.read_csv('USREC.csv')\n",
        "historical_recession_dataset['DATE'] = pd.to_datetime(historical_recession_dataset['DATE'])\n",
        "historical_recession_dataset['DATE'] = historical_recession_dataset['DATE'].dt.to_period(\"M\").dt.strftime(\"%m/%Y\")\n",
        "historical_recession_dataset = historical_recession_dataset[historical_recession_dataset['DATE'] >= start_date]\n",
        "\n",
        "# merge unemp + cpi\n",
        "merged_initial_dataset = pd.merge(unemp_dataset, cpi_dataset, on='DATE')\n",
        "\n",
        "# merge (unemp + cpi) + (t10y3m)\n",
        "merged_second_dataset = pd.merge(merged_initial_dataset, t_10y_3m_dataset, on='DATE')\n",
        "merged_second_dataset['DATE'] = pd.to_datetime(merged_second_dataset['DATE'])\n",
        "merged_second_dataset['DATE'] = merged_second_dataset['DATE'].dt.to_period(\"M\").dt.strftime(\"%m/%Y\")\n",
        "merged_second_dataset = merged_second_dataset[merged_second_dataset['DATE'] >= start_date]\n",
        "\n",
        "\n",
        "# merge (all indicators) + (sp500)\n",
        "merged_dataset = pd.merge(merged_second_dataset, sp_500_monthly_dataset, on=\"DATE\")\n",
        "\n",
        "\n",
        "# historical recession data + all indicators\n",
        "merged_with_actual = pd.merge(merged_dataset, historical_recession_dataset, on='DATE', how='left', suffixes=('_model', '_actual'))\n",
        "\n",
        "X = merged_dataset[['UNRATE', 'CPIAUCSL', 'T10Y3M', 'Adj Close']]\n",
        "y_actual = merged_with_actual['USREC']\n",
        "X = X.reset_index(drop=True)  # reset the index of X_test before splitting\n",
        "\n",
        "# 80% of the data is used to train the model, 20% is used to test\n",
        "X_train, X_test, y_train_actual, y_test_actual = train_test_split(X, y_actual, test_size=0.2, random_state=23)\n",
        "\n",
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_scaled_train = scaler.fit_transform(X_train)\n",
        "X_scaled_test = scaler.transform(X_test)\n",
        "X_scaled_test = pd.DataFrame(X_scaled_test, index=X_test.index) # reset the index of X_scaled_test after scaling\n",
        "X_test = X_test.reset_index(drop=True) # reset the index of X_test after scaling\n",
        "\n",
        "\n",
        "# Training\n",
        "class_weights = {0: 1, 1: 5.5}\n",
        "\n",
        "svm_model = SVC(kernel='rbf', C = 100, class_weight=class_weights, gamma = 1) # implemented kernel trick\n",
        "svm_model.fit(X_scaled_train, y_train_actual)\n",
        "\n",
        "'''\n",
        "# GridSearch Testing\n",
        "\n",
        "scorer = make_scorer(precision_score, pos_label=0)\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10],\n",
        "    'class_weight': [class_weights]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring=scorer)\n",
        "grid_search.fit(X_scaled_train, y_train_actual)\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Get the best model\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"Best SVM model\", best_svm_model)\n",
        "'''\n",
        "\n",
        "\n",
        "# Make predictions on the scaled set\n",
        "X_scaled = scaler.transform(X)\n",
        "merged_dataset['Recession_Predicted_Test'] = svm_model.predict(X_scaled)\n",
        "\n",
        "\n",
        "# Metrics for our model's predictions\n",
        "accuracy_merged = accuracy_score(y_actual, merged_dataset['Recession_Predicted_Test'])\n",
        "precision_merged = precision_score(y_actual, merged_dataset['Recession_Predicted_Test'], average = 'weighted')\n",
        "recall_merged = recall_score(y_actual, merged_dataset['Recession_Predicted_Test'], average = 'weighted')\n",
        "conf_matrix_merged = confusion_matrix(y_actual, merged_dataset['Recession_Predicted_Test'])\n",
        "f1 = f1_score(y_actual, merged_dataset['Recession_Predicted_Test'], average = 'weighted')\n",
        "roc_auc = roc_auc_score(y_actual, merged_dataset['Recession_Predicted_Test'])\n",
        "average_precision = average_precision_score(y_actual, merged_dataset['Recession_Predicted_Test'], average = 'weighted')\n",
        "\n",
        "print(\"Model's Prediction vs Actual Recessions:\")\n",
        "print(f\"Accuracy: {accuracy_merged * 100:.2f}%\")\n",
        "print(f\"Precision: {precision_merged * 100:.2f}%\")\n",
        "print(f\"Recall: {recall_merged * 100:.2f}%\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_merged}\") # how inaccurate is our model compared to real historical data\n",
        "print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
        "print(f\"ROC AUC Score: {roc_auc * 100:.2f}%\")"
      ]
    }
  ]
}